************************************************************************************************************************
Started at:  2023_09_21_19_57_23
VitClassificationDecomposed(
  (aggregators): ModuleList(
    (0): Aggregator(
      (all_models): ModuleDict(
        (class): TransformerBlockGroup(
          (blocks): ModuleDict(
            (transformer_block_0): TransformerBlock(
              (mha): MultiHeadAttention(
                (query_layer): Linear(in_features=768, out_features=768, bias=True)
                (key_layer): Linear(in_features=768, out_features=768, bias=True)
                (value_layer): Linear(in_features=768, out_features=768, bias=True)
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
              (layer_norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
              (pos_wise_ff_layer): PositionWiseFeedForward(
                (pos_wise_feed_forward): Sequential(
                  (0): Linear(in_features=768, out_features=3072, bias=True)
                  (1): GELU(approximate='none')
                  (2): Dropout(p=0.0, inplace=False)
                  (3): Linear(in_features=3072, out_features=768, bias=True)
                  (4): Dropout(p=0.0, inplace=False)
                )
              )
              (layer_norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
            (transformer_block_1): TransformerBlock(
              (mha): MultiHeadAttention(
                (query_layer): Linear(in_features=768, out_features=768, bias=True)
                (key_layer): Linear(in_features=768, out_features=768, bias=True)
                (value_layer): Linear(in_features=768, out_features=768, bias=True)
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
              (layer_norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
              (pos_wise_ff_layer): PositionWiseFeedForward(
                (pos_wise_feed_forward): Sequential(
                  (0): Linear(in_features=768, out_features=3072, bias=True)
                  (1): GELU(approximate='none')
                  (2): Dropout(p=0.0, inplace=False)
                  (3): Linear(in_features=3072, out_features=768, bias=True)
                  (4): Dropout(p=0.0, inplace=False)
                )
              )
              (layer_norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
            (transformer_block_2): TransformerBlock(
              (mha): MultiHeadAttention(
                (query_layer): Linear(in_features=768, out_features=768, bias=True)
                (key_layer): Linear(in_features=768, out_features=768, bias=True)
                (value_layer): Linear(in_features=768, out_features=768, bias=True)
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
              (layer_norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
              (pos_wise_ff_layer): PositionWiseFeedForward(
                (pos_wise_feed_forward): Sequential(
                  (0): Linear(in_features=768, out_features=3072, bias=True)
                  (1): GELU(approximate='none')
                  (2): Dropout(p=0.0, inplace=False)
                  (3): Linear(in_features=3072, out_features=768, bias=True)
                  (4): Dropout(p=0.0, inplace=False)
                )
              )
              (layer_norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
    )
    (1): Aggregator(
      (all_models): ModuleDict(
        (vehicle): TransformerBlockGroup(
          (blocks): ModuleDict(
            (transformer_block_0): TransformerBlock(
              (mha): MultiHeadAttention(
                (query_layer): Linear(in_features=768, out_features=768, bias=True)
                (key_layer): Linear(in_features=768, out_features=768, bias=True)
                (value_layer): Linear(in_features=768, out_features=768, bias=True)
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
              (layer_norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
              (pos_wise_ff_layer): PositionWiseFeedForward(
                (pos_wise_feed_forward): Sequential(
                  (0): Linear(in_features=768, out_features=3072, bias=True)
                  (1): GELU(approximate='none')
                  (2): Dropout(p=0.0, inplace=False)
                  (3): Linear(in_features=3072, out_features=768, bias=True)
                  (4): Dropout(p=0.0, inplace=False)
                )
              )
              (layer_norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
            (transformer_block_1): TransformerBlock(
              (mha): MultiHeadAttention(
                (query_layer): Linear(in_features=768, out_features=768, bias=True)
                (key_layer): Linear(in_features=768, out_features=768, bias=True)
                (value_layer): Linear(in_features=768, out_features=768, bias=True)
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
              (layer_norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
              (pos_wise_ff_layer): PositionWiseFeedForward(
                (pos_wise_feed_forward): Sequential(
                  (0): Linear(in_features=768, out_features=3072, bias=True)
                  (1): GELU(approximate='none')
                  (2): Dropout(p=0.0, inplace=False)
                  (3): Linear(in_features=3072, out_features=768, bias=True)
                  (4): Dropout(p=0.0, inplace=False)
                )
              )
              (layer_norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
            (transformer_block_2): TransformerBlock(
              (mha): MultiHeadAttention(
                (query_layer): Linear(in_features=768, out_features=768, bias=True)
                (key_layer): Linear(in_features=768, out_features=768, bias=True)
                (value_layer): Linear(in_features=768, out_features=768, bias=True)
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
              (layer_norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
              (pos_wise_ff_layer): PositionWiseFeedForward(
                (pos_wise_feed_forward): Sequential(
                  (0): Linear(in_features=768, out_features=3072, bias=True)
                  (1): GELU(approximate='none')
                  (2): Dropout(p=0.0, inplace=False)
                  (3): Linear(in_features=3072, out_features=768, bias=True)
                  (4): Dropout(p=0.0, inplace=False)
                )
              )
              (layer_norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (animal): TransformerBlockGroup(
          (blocks): ModuleDict(
            (transformer_block_0): TransformerBlock(
              (mha): MultiHeadAttention(
                (query_layer): Linear(in_features=768, out_features=768, bias=True)
                (key_layer): Linear(in_features=768, out_features=768, bias=True)
                (value_layer): Linear(in_features=768, out_features=768, bias=True)
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
              (layer_norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
              (pos_wise_ff_layer): PositionWiseFeedForward(
                (pos_wise_feed_forward): Sequential(
                  (0): Linear(in_features=768, out_features=3072, bias=True)
                  (1): GELU(approximate='none')
                  (2): Dropout(p=0.0, inplace=False)
                  (3): Linear(in_features=3072, out_features=768, bias=True)
                  (4): Dropout(p=0.0, inplace=False)
                )
              )
              (layer_norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
            (transformer_block_1): TransformerBlock(
              (mha): MultiHeadAttention(
                (query_layer): Linear(in_features=768, out_features=768, bias=True)
                (key_layer): Linear(in_features=768, out_features=768, bias=True)
                (value_layer): Linear(in_features=768, out_features=768, bias=True)
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
              (layer_norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
              (pos_wise_ff_layer): PositionWiseFeedForward(
                (pos_wise_feed_forward): Sequential(
                  (0): Linear(in_features=768, out_features=3072, bias=True)
                  (1): GELU(approximate='none')
                  (2): Dropout(p=0.0, inplace=False)
                  (3): Linear(in_features=3072, out_features=768, bias=True)
                  (4): Dropout(p=0.0, inplace=False)
                )
              )
              (layer_norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
            (transformer_block_2): TransformerBlock(
              (mha): MultiHeadAttention(
                (query_layer): Linear(in_features=768, out_features=768, bias=True)
                (key_layer): Linear(in_features=768, out_features=768, bias=True)
                (value_layer): Linear(in_features=768, out_features=768, bias=True)
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
              (layer_norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
              (pos_wise_ff_layer): PositionWiseFeedForward(
                (pos_wise_feed_forward): Sequential(
                  (0): Linear(in_features=768, out_features=3072, bias=True)
                  (1): GELU(approximate='none')
                  (2): Dropout(p=0.0, inplace=False)
                  (3): Linear(in_features=3072, out_features=768, bias=True)
                  (4): Dropout(p=0.0, inplace=False)
                )
              )
              (layer_norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
    )
  )
  (embedding_layer): Sequential(
    (0): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
    (1): Rearrange('b d ph pw -> b (ph pw) d')
  )
  (positional_embedding): PositionalEmbedding1D()
  (classifiers): ModuleDict(
    (aeroplane): Linear(in_features=768, out_features=10, bias=True)
    (ship): Linear(in_features=768, out_features=10, bias=True)
    (car): Linear(in_features=768, out_features=10, bias=True)
    (truck): Linear(in_features=768, out_features=10, bias=True)
    (dog): Linear(in_features=768, out_features=10, bias=True)
    (cat): Linear(in_features=768, out_features=10, bias=True)
    (deer): Linear(in_features=768, out_features=10, bias=True)
    (horse): Linear(in_features=768, out_features=10, bias=True)
    (bird): Linear(in_features=768, out_features=10, bias=True)
    (frog): Linear(in_features=768, out_features=10, bias=True)
  )
)
Experiment:  vit-decomposed-cifar10_1695306443
Training Started for ViT Decomposed model
Starting Train/Eval...
