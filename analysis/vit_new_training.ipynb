{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N+2 token train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CIFAR 10 Model Analysis\n",
    "# from numpy.random import choice\n",
    "\n",
    "# set_seed()\n",
    "\n",
    "# VALID_CLASSES = [\"ALL\"]\n",
    "\n",
    "# transform = transforms.Compose(\n",
    "#     [\n",
    "#         transforms.PILToTensor(),\n",
    "#         transforms.ConvertImageDtype(torch.float),\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "# dataset = CIFAR10(\".\", download=True, transform=transform)\n",
    "\n",
    "# mask_indices = [idx for idx, s in enumerate(dataset) if s[1] in VALID_CLASSES]\n",
    "# mask_indices = choice(mask_indices, 1000, replace=False)\n",
    "# dataset = Subset(dataset, mask_indices)\n",
    "# # train_size = int(0.02 * len(dataset))\n",
    "# train_size = 1\n",
    "# BATCH_SIZE = 1\n",
    "\n",
    "# train_ds, x_ds = random_split(dataset, [train_size, len(dataset) - train_size])\n",
    "\n",
    "# print(\"Number of samples in dataset: \", len(train_ds))\n",
    "\n",
    "# del dataset\n",
    "# del x_ds\n",
    "\n",
    "# train_dl = DataLoader(train_ds, BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n",
    "# PRETRAINED_MODEL_NAME = \"aaraki/vit-base-patch16-224-in21k-finetuned-cifar10\"  # Pretrained on ImageNet, FineTuned on CIFAR 10\n",
    "# FEAT_EXTRACT = ViTImageProcessor()\n",
    "# VIT_MODEL = to_device(\n",
    "#     ViTForImageClassification.from_pretrained(PRETRAINED_MODEL_NAME), device\n",
    "# )\n",
    "\n",
    "# def get_pretrained_vit_outputs(batch: tuple):\n",
    "#     with torch.no_grad():\n",
    "\n",
    "#         ip_imgs = batch[0]\n",
    "#         ip_labels = batch[1]\n",
    "\n",
    "#         inputs = to_device(\n",
    "#             FEAT_EXTRACT.preprocess(\n",
    "#                 images=ip_imgs, data_format=\"channels_first\", return_tensors=\"pt\"\n",
    "#             )[\"pixel_values\"],\n",
    "#             device,\n",
    "#         )\n",
    "#         outputs = VIT_MODEL(inputs, output_attentions=True, output_hidden_states=True)\n",
    "#         preds = outputs.logits.argmax(dim=1)\n",
    "\n",
    "#         return {\"hidden_states\": outputs.hidden_states}\n",
    "\n",
    "# all_images_data = {}\n",
    "# # Processing for [START_LAYER, END_LAYER)\n",
    "# START_LAYER = 0\n",
    "# END_LAYER = 13\n",
    "# NUM_LAYERS = END_LAYER - START_LAYER\n",
    "# TOTAL_LAYERS = 13 # +1 for starting embedding dimension\n",
    "#     # for batch in train_dl:\n",
    "#     #     ops = get_pretrained_vit_outputs(batch)\n",
    "\n",
    "#     #     for idx in range(START_LAYER, END_LAYER):\n",
    "#     #         all_images_data[label][\"hidden_states\"][idx] = torch.cat(\n",
    "#     #             [\n",
    "#     #                 all_images_data[label][\"hidden_states\"][idx],\n",
    "#     #                 ops[\"hidden_states\"][idx].detach().cpu(),\n",
    "#     #             ]\n",
    "#     #         )\n",
    "\n",
    "#     # print(\"Processing for label complete: \", label)\n",
    "\n",
    "# def get_layer_variance(t_var: torch.Tensor) -> torch.Tensor:\n",
    "#     # (batch_size B, seq_len S, emb_dim D)\n",
    "#     if t_var.shape[0] != 1:\n",
    "#         t_var = torch.var(t_var, dim=(0,), keepdim=True)\n",
    "\n",
    "#     # Remove CLS\n",
    "#     t_var = t_var[0, 1:, :]\n",
    "\n",
    "#     # Mean along D\n",
    "#     t_var = torch.mean(t_var, dim=(1,))\n",
    "#     t_var = einops.rearrange(t_var, \"(h w) -> h w\", h=14, w=14)\n",
    "#     t_var = einops.repeat(t_var, \"x y -> (x p1) (y p2)\", x=14, y=14, p1=16, p2=16)\n",
    "\n",
    "#     return t_var\n",
    "# for label in VALID_CLASSES:\n",
    "#     # class_labels = torch.tensor([label])\n",
    "#     print(f\"Processing for Label: {label} for layers {START_LAYER} to {END_LAYER}\")\n",
    "\n",
    "#     all_images_data[label] = {\n",
    "#         # \"hidden_states\": [torch.empty(0, 197, 768) for _ in range(TOTAL_LAYERS)],\n",
    "#         \"variance\": [None for _ in range(NUM_LAYERS)],\n",
    "#     }\n",
    "\n",
    "#     for layer_idx in range(START_LAYER, END_LAYER):\n",
    "#         # RAM crash so redundancy\n",
    "#         layer_variance = torch.empty(0, 197, 768)\n",
    "\n",
    "#         for batch in train_dl:\n",
    "#             layer_hidden_state = get_pretrained_vit_outputs(batch)[\"hidden_states\"][layer_idx].detach().cpu()\n",
    "#             layer_variance = torch.cat([layer_variance, layer_hidden_state])\n",
    "\n",
    "#         layer_variance = get_layer_variance(layer_variance)\n",
    "\n",
    "#         all_images_data[label][\"variance\"][layer_idx] = layer_variance\n",
    "# from torchvision.transforms.functional import resize\n",
    "\n",
    "# # 1000 images for all classes\n",
    "# import matplotlib as mpl\n",
    "\n",
    "# mpl.rcParams[\"figure.dpi\"] = 100\n",
    "\n",
    "# t_var = all_images_data[label][\"variance\"][2]\n",
    "# img = resize(train_ds[0][0], size=(224, 224), antialias=True)\n",
    "# plt.imshow(img.permute(1, 2, 0))\n",
    "# plt.imshow(t_var, cmap=\"gray\", alpha=0.2)\n",
    "# # # One class at a time\n",
    "\n",
    "# # fig, axs = plt.subplots(NUM_LAYERS, figsize=(10, 15))\n",
    "# # fig.suptitle(f\"Variance along seq for class: {VALID_CLASSES}\")\n",
    "# # fig.tight_layout()\n",
    "# # class_label = VALID_CLASSES[0]\n",
    "\n",
    "# # for idx, state in enumerate(\n",
    "# #     all_images_data[class_label][\"hidden_states\"][START_LAYER:END_LAYER]\n",
    "# # ):\n",
    "# #     # (B, S, D) -> (S)\n",
    "# #     op = torch.var(state, dim=(0, 2))\n",
    "\n",
    "# #     axs[idx].plot(op)\n",
    "# #     axs[idx].set_title(f\"Layer: {idx + START_LAYER}\", fontsize=10, loc=\"right\")\n",
    "\n",
    "# # plt.show()\n",
    "# # fig, axs = plt.subplots(NUM_LAYERS, figsize=(10, 15))\n",
    "# # fig.suptitle(f\"Variance along patch for class: {VALID_CLASSES}\")\n",
    "# # fig.tight_layout()\n",
    "# # class_label = VALID_CLASSES[0]\n",
    "\n",
    "# # for idx, state in enumerate(\n",
    "# #     all_images_data[class_label][\"hidden_states\"][START_LAYER:END_LAYER]\n",
    "# # ):\n",
    "# #     # (B, S, D) -> (S)\n",
    "# #     op = torch.var(state, dim=(0, 1))\n",
    "\n",
    "# #     axs[idx].plot(op)\n",
    "# #     axs[idx].set_title(f\"Layer: {idx + START_LAYER}\", fontsize=10, loc=\"right\")\n",
    "\n",
    "# # plt.show()\n",
    "\n",
    "# # 1000 images for all classes\n",
    "# import matplotlib as mpl\n",
    "\n",
    "# mpl.rcParams[\"figure.dpi\"] = 400\n",
    "\n",
    "# fig, axs = plt.subplots(NUM_LAYERS, figsize=(16, 20), sharex=True)\n",
    "# title = f\"ViT - Variance Patch wise 1 image CIFAR-10 (224 x 224)\"\n",
    "# fig.tight_layout()\n",
    "\n",
    "# img = resize(train_ds[0][0], size=(224, 224), antialias=True).permute(1, 2, 0)\n",
    "\n",
    "# for idx, state in enumerate(all_images_data[VALID_CLASSES[0]][\"variance\"]):\n",
    "#     print(\"Processing for Layer: \", idx)\n",
    "#     # Normalized Variance\n",
    "#     # tensor_variance = 20 * torch.log10(1 + state)\n",
    "#     tensor_variance = state\n",
    "\n",
    "#     axs[idx].set_title(f\"Depth: {START_LAYER + idx}\", fontsize=10, loc=\"right\")\n",
    "#     axs[idx].xaxis.set_visible(False)\n",
    "#     axs[idx].yaxis.set_visible(False)\n",
    "#     axs[idx].imshow(img, alpha=1.0)\n",
    "\n",
    "#     im = axs[idx].imshow(tensor_variance, cmap=\"jet\", alpha=0.5)\n",
    "#     plt.colorbar(im, ax=axs[idx])\n",
    "\n",
    "\n",
    "# axs[5].set_title(title, rotation=\"vertical\", x=-0.1, y=0.5, fontsize=10)\n",
    "\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sandip-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
