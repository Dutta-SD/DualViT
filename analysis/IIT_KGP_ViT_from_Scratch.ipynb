{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "i9iSd-WV7of8"
   },
   "source": [
    "# Vision Transformer\n",
    "- [Paper Link](https://arxiv.org/pdf/2010.11929v2.pdf)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Mb5nlSfza1Wv"
   },
   "source": [
    "## 1. Model from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fJsL5xun6ghq"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import Tensor\n",
    "from einops.layers.torch import Rearrange\n",
    "import numpy as np\n",
    "from torch.nn import functional as F\n",
    "import einops\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torch.utils.data import random_split\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Subset\n",
    "from transformers import ViTForImageClassification, ViTImageProcessor\n",
    "from PIL import Image\n",
    "import requests\n",
    "import random\n",
    "import os\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "WEIGHT_FOLDER_PATH = \"./checkpoints\"\n",
    "BATCH_SIZE = 256\n",
    "LEARNING_RATE = 1e-3\n",
    "WEIGHT_DECAY = 1e-5\n",
    "EPOCHS = 300\n",
    "MODEL_SAVE_NAME = f\"{WEIGHT_FOLDER_PATH}/CURR-BEST-ViT-MODEL-CIFAR-10-BROAD-FINE.pt\"\n",
    "\n",
    "\n",
    "def set_seed(seed: int = 42) -> None:\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    # When running on the CuDNN backend, two further options must be set\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    # Set a fixed value for the hash seed\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    print(f\"Random seed set as {seed}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar_10_labels = {0: {}, 1: {}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(outputs, labels):\n",
    "    _, preds = torch.max(outputs, dim=1)\n",
    "    return torch.tensor(torch.sum(preds == labels).item() / len(preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "48p7kpDL6s6-"
   },
   "outputs": [],
   "source": [
    "class ImageClassificationBase(nn.Module):\n",
    "    \"\"\"\n",
    "    Base class containing utilities\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.acc = -1\n",
    "\n",
    "    def training_step(self, batch):\n",
    "        images, labels_fine, labels_broad = batch\n",
    "        broad_out, fine_out = self(images)  # Generate predictions\n",
    "        loss1 = F.cross_entropy(fine_out, labels_fine)  # Calculate loss\n",
    "        loss2 = F.cross_entropy(broad_out, labels_broad)  # Calculate loss\n",
    "        return loss1, loss2\n",
    "\n",
    "    def validation_step(self, batch):\n",
    "        images, labels_fine, labels_broad = batch\n",
    "        broad_out, fine_out = self(images)  # Generate predictions\n",
    "\n",
    "        loss1 = F.cross_entropy(fine_out, labels_fine)  # Calculate loss\n",
    "        loss2 = F.cross_entropy(broad_out, labels_broad)  # Calculate loss\n",
    "\n",
    "        acc_fine = accuracy(fine_out, labels_fine)\n",
    "        acc_broad = accuracy(broad_out, labels_broad)\n",
    "\n",
    "        return {\n",
    "            \"val_loss_fine\": loss1.detach(),\n",
    "            \"val_loss_broad\": loss2.detach(),\n",
    "            \"val_acc_fine\": acc_fine,\n",
    "            \"val_acc_broad\": acc_broad,\n",
    "        }\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        # Fine Label Validation\n",
    "        batch_losses = [x[\"val_loss_fine\"] for x in outputs]\n",
    "        epoch_loss = torch.stack(batch_losses).mean()  # Combine losses\n",
    "\n",
    "        batch_accs = [x[\"val_acc_fine\"] for x in outputs]\n",
    "        epoch_acc = torch.stack(batch_accs).mean()  # Combine accuracies\n",
    "\n",
    "        # Broad Label Validation\n",
    "        batch_losses_broad = [x[\"val_loss_broad\"] for x in outputs]\n",
    "        epoch_loss_broad = torch.stack(batch_losses_broad).mean()  # Combine losses\n",
    "\n",
    "        batch_accs_broad = [x[\"val_acc_broad\"] for x in outputs]\n",
    "        epoch_acc_broad = torch.stack(batch_accs_broad).mean()  # Combine losses\n",
    "\n",
    "        return {\n",
    "            \"val_loss_fine\": epoch_loss.item(),\n",
    "            \"val_acc_fine\": epoch_acc.item(),\n",
    "            \"val_loss_broad\": epoch_loss_broad.item(),\n",
    "            \"val_acc_broad\": epoch_acc_broad.item(),\n",
    "        }\n",
    "\n",
    "    def reset_stored_accuracy(self):\n",
    "        self.acc = -1\n",
    "\n",
    "    def epoch_end(self, epoch, result, mode):\n",
    "        print(f\"Epoch: [{epoch:<5}]\")\n",
    "        print(f\"Train Loss Fine: {result['train_loss_fine']:.5f}\")\n",
    "        print(f\"Val Loss Fine: {result['val_loss_fine']:.5f}\")\n",
    "        print(f\"Val Acc Fine: {result['val_acc_fine']:.5f}\")\n",
    "\n",
    "        print(f\"Train Loss Broad: {result['train_loss_broad']:.5f}\")\n",
    "        print(f\"Val Loss Broad: {result['val_loss_broad']:.5f}\")\n",
    "        print(f\"Val Acc Broad: {result['val_acc_broad']:.5f}\")\n",
    "\n",
    "        if epoch % 25 == 0:\n",
    "            torch.save(self, f\"./checkpoints/Vit-{datetime.now()}.pt\")\n",
    "            print(f\"\\nCheckpoint saved at ./checkpoints/Vit-{datetime.now()}.pt\")\n",
    "\n",
    "        mode_label = None\n",
    "\n",
    "        if mode == \"BROAD_ONLY\":\n",
    "            mode_label = \"val_acc_broad\"\n",
    "        elif mode_label == \"FINE_ONLY\":\n",
    "            label = \"val_acc_fine\"\n",
    "        elif mode == \"BROAD_AND_FINE\":\n",
    "            pass\n",
    "\n",
    "        # TODO: This stays as is. Change to a separate function once needed\n",
    "        # Keep this and for now\n",
    "\n",
    "        if result[mode_label] > self.acc:\n",
    "            print(\n",
    "                f\"{mode_label} Validation Accuracy Increased from {self.acc} to {result[mode_label]}\"\n",
    "            )\n",
    "            self.acc = result[mode_label]\n",
    "            save_path = MODEL_SAVE_NAME\n",
    "            torch.save(self, save_path)\n",
    "            print(f\"Model Saved @ {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x8mkNaOMoEhk"
   },
   "outputs": [],
   "source": [
    "class PositionalEmbedding1D(nn.Module):\n",
    "    \"\"\"\n",
    "    Adds (optionally learned) positional embeddings to the inputs\n",
    "    When using additional classification token, seq_len will be sequence length + 1\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, seq_len, d_model):\n",
    "        super().__init__()\n",
    "        self.pos_embedding = nn.Parameter(torch.zeros(1, seq_len, d_model))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pos_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi Head Attention Block.\n",
    "    Multi Head, so splits and rejoins\n",
    "    Takes in a tensor of shape (seq_len, emb_dim) and computes query, key, values\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, emb_dim, num_heads, dropout_proba):\n",
    "        super().__init__()\n",
    "        self.emb_dim = emb_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.query_layer = nn.Linear(emb_dim, emb_dim)\n",
    "        self.key_layer = nn.Linear(emb_dim, emb_dim)\n",
    "        self.value_layer = nn.Linear(emb_dim, emb_dim)\n",
    "        self.dropout = nn.Dropout(dropout_proba)\n",
    "        self.scores = None\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        x, q(query), k(key), v(value) : (B(batch_size), S(seq_len), D(dim))\n",
    "        mask : (B(batch_size) x S(seq_len))\n",
    "        * split D(dim) into (H(n_heads), W(width of head)) ; D = H * W\n",
    "        \"\"\"\n",
    "        # (B, S, D) -proj-> (B, S, D) -split-> (B, S, numH, W_head) -trans-> (B, numH, S, W_head)\n",
    "        q, k, v = self.query_layer(x), self.key_layer(x), self.value_layer(x)\n",
    "\n",
    "        q, k, v = (\n",
    "            einops.rearrange(i, \"b s (nh wh) -> b nh s wh\", nh=self.num_heads)\n",
    "            for i in [q, k, v]\n",
    "        )\n",
    "        # (B, H, S, W) @ (B, H, W, S) -> (B, H, S, S) -softmax-> (B, H, S, S)\n",
    "        scores = q @ k.transpose(-2, -1) / np.sqrt(k.size(-1))\n",
    "        if mask is not None:\n",
    "            mask = mask[:, None, None, :].float()\n",
    "            scores -= 10000.0 * (1.0 - mask)\n",
    "        scores = self.dropout(F.softmax(scores, dim=-1))\n",
    "        # (B, H, S, S) @ (B, H, S, W) -> (B, H, S, W) -trans-> (B, S, H, W)\n",
    "        h = einops.rearrange(scores @ v, \"b h s w -> b s h w\")\n",
    "        # -merge-> (B, S, D)\n",
    "        h = einops.rearrange(h, \"b s h w -> b s (h w)\")\n",
    "        self.scores = scores\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionWiseFeedForward(nn.Module):\n",
    "    \"\"\"\n",
    "    FeedForward Neural Networks for each\n",
    "    element of the seuqence\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, emb_dim, feed_fwd_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(emb_dim, feed_fwd_dim)\n",
    "        self.fc2 = nn.Linear(feed_fwd_dim, emb_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # (B, S, D) -> (B, S, D_ff) -> (B, S, D)\n",
    "        return self.fc2(F.gelu(self.fc1(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Single Block of Transformer with Residual Connection\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim, num_heads, ff_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.attn = MultiHeadSelfAttention(dim, num_heads, dropout)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.norm1 = nn.LayerNorm(dim, eps=1e-6)\n",
    "        self.pwff = PositionWiseFeedForward(dim, ff_dim)\n",
    "        self.norm2 = nn.LayerNorm(dim, eps=1e-6)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        h = self.drop(self.proj(self.attn(self.norm1(x), mask)))\n",
    "        x = x + h\n",
    "        h = self.drop(self.pwff(self.norm2(x)))\n",
    "        x = x + h\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer with Self-Attentive Blocks\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_layers, emb_dim, num_heads, ff_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.blocks = nn.ModuleList(\n",
    "            [\n",
    "                TransformerBlock(emb_dim, num_heads, ff_dim, dropout)\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        for block in self.blocks:\n",
    "            x = block(x, mask)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "muYttWe_80I7"
   },
   "outputs": [],
   "source": [
    "class VisionTransformer(ImageClassificationBase):\n",
    "    \"\"\"\n",
    "    Main module for Vision Transformer for 2D colour images\n",
    "    We will use the Conv2D hybrid architecture\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        img_height: int,\n",
    "        img_width: int,\n",
    "        patch_dim: int,\n",
    "        emb_dim: int,\n",
    "        num_classes: int,\n",
    "        in_channels: int = 3,\n",
    "        num_heads: int = 12,\n",
    "        pwff_dim: int = 3072,\n",
    "        num_layers: int = 12,\n",
    "        dropout: float = 0.6,\n",
    "        num_broad_classes: int = 2,  # default for CIFAR 10\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.img_height = img_height\n",
    "        self.img_width = img_width\n",
    "        self.patch_dim = patch_dim\n",
    "        self.emb_dim = emb_dim\n",
    "        self.num_patch_width = img_width // patch_dim\n",
    "        self.num_patch_height = img_height // patch_dim\n",
    "        # +2 for 2 additional tokens, one for broad class one for fine class\n",
    "        self.seq_len = (self.num_patch_width * self.num_patch_height) + 2\n",
    "\n",
    "        # (b, c, h, w) -> (b, nh*nw, d)\n",
    "        self.embedding_layer = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels,\n",
    "                emb_dim,\n",
    "                kernel_size=(patch_dim, patch_dim),\n",
    "                stride=(patch_dim, patch_dim),\n",
    "            ),\n",
    "            Rearrange(\"b d x y -> b (x y) d\"),\n",
    "        )\n",
    "\n",
    "        # class tokens\n",
    "        self.fine_class_token = nn.Parameter(torch.zeros(1, 1, self.emb_dim))\n",
    "        self.broad_class_token = nn.Parameter(torch.zeros(1, 1, self.emb_dim))\n",
    "\n",
    "        # 1D positional embedding\n",
    "        self.positional_embedding = PositionalEmbedding1D(self.seq_len, self.emb_dim)\n",
    "        self.transformer_encoder = TransformerEncoder(\n",
    "            num_layers, emb_dim, num_heads, pwff_dim, dropout\n",
    "        )\n",
    "        # Implement classification\n",
    "        self.norm = nn.LayerNorm(emb_dim)\n",
    "        self.mlp_fine = nn.Linear(emb_dim, num_classes)\n",
    "        self.mlp_broad = nn.Linear(emb_dim, num_broad_classes)\n",
    "\n",
    "    def add_class_tokens_to_input(self, x):\n",
    "        \"\"\"\n",
    "        Adds [class] token to the input x\n",
    "        \"\"\"\n",
    "        bs = x.shape[0]\n",
    "\n",
    "        fine_class_token_expanded = einops.repeat(\n",
    "            self.fine_class_token, \"1 s d -> bs s d\", bs=bs\n",
    "        )\n",
    "        broad_class_token_expanded = einops.repeat(\n",
    "            self.broad_class_token, \"1 s d -> bs s d\", bs=bs\n",
    "        )\n",
    "\n",
    "        return torch.cat(\n",
    "            [x, fine_class_token_expanded, broad_class_token_expanded], dim=1\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.embedding_layer(x)\n",
    "        y = self.add_class_tokens_to_input(y)\n",
    "        y = self.positional_embedding(y)\n",
    "        y = self.transformer_encoder(y)\n",
    "\n",
    "        # Get [class] tokens\n",
    "        # last token is fine class token\n",
    "        fine_class_token = self.norm(y)[:, -1]\n",
    "        broad_class_token = self.norm(y)[:, -2]\n",
    "\n",
    "        # Return fine and broad tokens from MLP head\n",
    "        return (\n",
    "            self.mlp_broad(broad_class_token),\n",
    "            self.mlp_fine(fine_class_token),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BefYyboHNW1Q"
   },
   "outputs": [],
   "source": [
    "def get_default_device():\n",
    "    \"\"\"Pick GPU if available, else CPU\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    else:\n",
    "        return torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_device(data, device):\n",
    "    \"\"\"Move tensor(s) to chosen device\"\"\"\n",
    "    if isinstance(data, (list, tuple)):\n",
    "        return [to_device(x, device) for x in data]\n",
    "    return data.to(device, non_blocking=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeviceDataLoader:\n",
    "    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n",
    "\n",
    "    def __init__(self, dl, device):\n",
    "        self.dl = dl\n",
    "        self.device = device\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\"Yield a batch of data after moving it to device\"\"\"\n",
    "        for b in self.dl:\n",
    "            yield to_device(b, self.device)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Number of batches\"\"\"\n",
    "        return len(self.dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a6ine2HrodLO",
    "outputId": "272f4b22-4756-4aad-93d2-c06babfdc99d"
   },
   "outputs": [],
   "source": [
    "device = get_default_device()\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VitKVIxqqd-3"
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(model, val_loader, mode):\n",
    "    model.eval()\n",
    "    outputs = [model.validation_step(batch) for batch in val_loader]\n",
    "    return model.validation_epoch_end(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(epochs, model, train_loader, val_loader, optimizer, mode):\n",
    "    \"\"\"\n",
    "    epochs - number of epochs\n",
    "    model - the model in required device\n",
    "    train_loader - train dataloader\n",
    "    val_loader - val dataloader\n",
    "    optimizer - optimizer, with parameters and parameters set\n",
    "    \"\"\"\n",
    "    history = []\n",
    "    for epoch in range(epochs):\n",
    "        # Training Phase\n",
    "        model.train()\n",
    "        train_losses_broad = []\n",
    "        train_losses_fine = []\n",
    "\n",
    "        for batch in train_loader:\n",
    "            loss_fine, loss_broad = model.training_step(batch)\n",
    "            train_losses_broad.append(loss_broad)\n",
    "            train_losses_fine.append(loss_fine)\n",
    "\n",
    "            # loss_fine.backward(retain_graph=True)\n",
    "            loss_broad.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        # Validation phase\n",
    "        result = evaluate(model, val_loader, mode)\n",
    "\n",
    "        result[\"train_loss_fine\"] = torch.stack(train_losses_fine).mean().item()\n",
    "        result[\"train_loss_broad\"] = torch.stack(train_losses_broad).mean().item()\n",
    "\n",
    "        model.epoch_end(epoch, result, mode)\n",
    "        history.append(result)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5NSBsLskoQzC"
   },
   "outputs": [],
   "source": [
    "from numpy.random import choice\n",
    "\n",
    "class_label = [i for i in range(10)]\n",
    "\n",
    "device = get_default_device()\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.PILToTensor(),\n",
    "        transforms.ConvertImageDtype(torch.float),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Tuple\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.datasets import CIFAR10\n",
    "\n",
    "\n",
    "def get_broad_label(idx):\n",
    "    fine_to_broad = [0, 0, 1, 1, 1, 1, 1, 1, 0, 0]\n",
    "    return fine_to_broad[idx]\n",
    "\n",
    "\n",
    "class CIFAR10MultiLabelDataset(CIFAR10):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def __len__(self):\n",
    "        return super().__len__()\n",
    "\n",
    "    def __getitem__(self, index: int) -> Tuple[Any, Any]:\n",
    "        img_tensor, fine_label = super().__getitem__(index)\n",
    "        return img_tensor, fine_label, get_broad_label(fine_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CIFAR10MultiLabelDataset(\".\", download=True, transform=transform)\n",
    "\n",
    "TRAIN_DATA_SIZE = 25000\n",
    "VAL_DATA_SIZE = 10000\n",
    "REMAINING = len(dataset) - TRAIN_DATA_SIZE - VAL_DATA_SIZE\n",
    "\n",
    "train_ds, val_ds, _ = random_split(dataset, [TRAIN_DATA_SIZE, VAL_DATA_SIZE, REMAINING])\n",
    "\n",
    "len(train_ds), len(val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  DataLoaders\n",
    "train_dl = DataLoader(\n",
    "    train_ds, BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True\n",
    ")\n",
    "val_dl = DataLoader(val_ds, BATCH_SIZE * 2, num_workers=2, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Mode:\n",
    "    FINE_ONLY: str = \"FINE_ONLY\"\n",
    "    BROAD_ONLY: str = \"BROAD_ONLY\"\n",
    "    FINE_AND_BROAD: str = \"FINE_AND_BROAD\"\n",
    "\n",
    "\n",
    "all_modes = Mode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XDdeiK5pqrwU"
   },
   "outputs": [],
   "source": [
    "# Experiment 2\n",
    "model = VisionTransformer(\n",
    "    img_height=32,\n",
    "    img_width=32,\n",
    "    patch_dim=8,\n",
    "    emb_dim=768,\n",
    "    num_classes=10,\n",
    "    in_channels=3,\n",
    "    num_heads=8,\n",
    "    num_layers=8,\n",
    "    pwff_dim=3072,\n",
    "    dropout=0.5,\n",
    "    num_broad_classes=2,  # default for CIFAR 10\n",
    ")\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOAD_MODEL = False\n",
    "\n",
    "if LOAD_MODEL:\n",
    "    print(\"loading saved\")\n",
    "    model = torch.load(MODEL_SAVE_NAME)\n",
    "\n",
    "model = to_device(model, device)\n",
    "print(f\"Last Saved Model Validation Accuracy: {model.acc}\")\n",
    "\n",
    "train_dl, val_dl = DeviceDataLoader(train_dl, device), DeviceDataLoader(val_dl, device)\n",
    "\n",
    "# Sanity Test\n",
    "evaluate(model, val_dl, all_modes.BROAD_ONLY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(\n",
    "    model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY\n",
    ")\n",
    "# Parameters\n",
    "EPOCHS = 300\n",
    "\n",
    "history = fit(EPOCHS, model, train_dl, val_dl, optimizer, all_modes.BROAD_ONLY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(model, val_dl, all_modes.BROAD_ONLY)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
